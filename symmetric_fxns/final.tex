\documentclass[8pt]{extarticle}
%Some packages I commonly use.
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{framed}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{ytableau}
\usepackage[utf8]{inputenc}
\usepackage[top=1 in,bottom=1in, left=1 in, right=1 in]{geometry}

%A bunch of definitions that make my life easier
\newcommand{\matlab}{{\sc Matlab} }
\newcommand{\abs}[1]{|#1|}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\cvec}[1]{{\mathbf #1}}
\newcommand{\rvec}[1]{\vec{\mathbf #1}}
\newcommand{\ihat}{\hat{\textbf{\i}}}
\newcommand{\jhat}{\hat{\textbf{\j}}}
\newcommand{\khat}{\hat{\textbf{k}}}
\newcommand{\minor}{{\rm minor}}
\newcommand{\trace}{{\rm trace}}
\newcommand{\spn}{{\rm Span}}
\newcommand{\rem}{{\rm rem}}
\newcommand{\ran}{{\rm range}}
\newcommand{\range}{{\rm range}}
\newcommand{\mdiv}{{\rm div}}
\newcommand{\proj}{{\rm proj}}
\newcommand{\sgn}{\text{sgn}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\renewcommand{\emptyset}{\varnothing}
\newcommand{\attn}[1]{\textbf{#1}}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{prob}{Problem}
\newtheorem{corollary}{Corollary}
\newtheorem*{definition}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{note}{Note}
\newtheorem{exercise}{Exercise}
\newcommand{\bproof}{\bigskip {\bf Proof. }}
\newcommand{\eproof}{\hfill\qedsymbol}
\newcommand{\Disp}{\displaystyle}
\newcommand{\qe}{\hfill\(\bigtriangledown\)}
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}
\setlength{\columnseprule}{1 pt}

\title{ Math 4995/5327
  \\
  Final Exam}
\author{David Draguta}

\begin{document}

\maketitle

\begin{exercise}
  Prove that, for all $0 \leq k \leq n$, we have
  \begin{align*}
    e_k(\underbrace{1,\dots, 1}_{\text{n entries}}) = 
    \begin{pmatrix}
      n \\
      k
    \end{pmatrix}
    \quad \text{ and } \quad
    h_k(\underbrace{1,\dots, 1}_{\text{n entries}}) = 
    \begin{pmatrix}
      n + k - 1 \\
      k
    \end{pmatrix}.  
  \end{align*}

\end{exercise}

\begin{proof}
  By Proposition 2.4.5 we have for $0 \leq k \leq n$: \footnote{Where $e_k = e_ke_0 = e_{(k)}$. }
  \begin{align*}
    e_k(\underbrace{1, \dots, 1}_{\text{n entries}}) = \sum\limits_{T \in RowStrict((k), n)} 1 =
    \begin{pmatrix}
      n \\
      k
    \end{pmatrix},
  \end{align*}
  since a strictly increasing row of $k$ boxes with entries from $1$ to $n$ corresponds to a choice of $k$ distinct numbers from $1$ to $n$ up to permutation, and there are
  $\begin{pmatrix}
      n \\
      k
  \end{pmatrix}$
  such choices, i.e. that many tableaux.

  Next, arguing by induction, let $0 \leq k \leq n$, then 
  \begin{align*}
    h_k(x_1, \dots, x_n)
    &= h_k(x_1, \dots, x_{n-1}) + x_n h_{k-1}(x_1, \dots, x_n),
  \end{align*}
  so
  \begin{align*}
    h_k(\underbrace{1, \dots, 1}_{n \text{ entries} })
    &= h_k(\underbrace{1, \dots, 1}_{n-1 \text{ entries} }) + h_{k-1}(\underbrace{1, \dots, 1}_{n \text{ entries }}) \\
    &=
    \begin{pmatrix}
      (n-1) + k - 1 \\
      k
    \end{pmatrix}
    +
    \begin{pmatrix}
      n + (k-1) - 1 \\
      k - 1
    \end{pmatrix}
    \\
    &=
    \begin{pmatrix}
      n + k - 2 \\
      k
    \end{pmatrix}
    +
    \begin{pmatrix}
      n + k - 2 \\
      k - 1
    \end{pmatrix}
    \\
    &=    
    \begin{pmatrix}
      n + k - 1 \\
      k
    \end{pmatrix},
  \end{align*}
  with base case $k=0$:
  \begin{align*}
    h_0(\underbrace{1, \dots, 1}_{n \text{ entries }}) = 1 =
    \begin{pmatrix}
      n + 0 - 1 \\
      0
    \end{pmatrix}.
  \end{align*}
\end{proof}
\begin{exercise}
  Using Newton's identity
  \begin{align*}
    ne_n = \sum\limits_{r=1}^{n}(-1)^{r-1}p_r e_{n-r}
  \end{align*}
  prove that
  \begin{align*}
    p_n = 
    \begin{vmatrix}
      e_1 & 1 & 0 & 0 & \cdots & 0 \\
      2e_2 & e_1 & 1 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & 1 & 0 \\
      \vdots & \vdots & \ddots & \ddots  & e_1 & 1 \\            
      ne_n & e_{n-1} & e_{n-2} & \cdots  & e_2 & e_1 \\            
    \end{vmatrix}
    \quad \text { and } \quad
    n!e_n = 
    \begin{vmatrix}
      p_1 & 1 & 0 & 0 & \cdots & 0 \\
      p_2 & p_1 & 2 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & 0 \\
      p_{n-1} & p_{n-2} & \cdots & \cdots  & p_1 & n-1 \\            
      p_n & p_{n-1} & \cdots & \cdots  & p_2 & p_1 \\            
    \end{vmatrix}    
  \end{align*}
\end{exercise}
\begin{proof}
  \begin{align*}
    \begin{vmatrix}
      e_1 & 1 & 0 & 0 & \cdots & 0 \\
      2e_2 & e_1 & 1 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & 1 & 0 \\
      \vdots & \vdots & \ddots & \ddots  & e_1 & 1 \\            
      ne_n & e_{n-1} & e_{n-2} & \cdots  & e_2 & e_1 \\            
    \end{vmatrix}
    &=
    \begin{vmatrix}
      \sum\limits_{r=1}^{1}(-1)^{r-1}p_r e_{1-r} & 1 & 0 & 0 & \cdots & 0 \\
      \sum\limits_{r=1}^{2}(-1)^{r-1}p_r e_{2-r} & e_1 & 1 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & 1 & 0 \\
      \vdots & \vdots & \ddots & \ddots  & e_1 & 1 \\            
      \sum\limits_{r=1}^{n}(-1)^{r-1}p_r e_{n-r} & e_{n-1} & e_{n-2} & \cdots  & e_2 & e_1 \\            
    \end{vmatrix}    \\
    &=p_1
    \begin{vmatrix}
      1 & 1 & 0 & 0 & \cdots & 0 \\
      e_1 & e_1 & 1 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & 1 & 0 \\
      \vdots & \vdots & \ddots & \ddots  & e_1 & 1 \\            
      e_{n-1} & e_{n-1} & e_{n-2} & \cdots  & e_2 & e_1 \\
    \end{vmatrix} \\
    &+
    \begin{vmatrix}
      0 & 1 & 0 & 0 & \cdots & 0 \\
      \sum\limits_{r=2}^{2}(-1)^{r-1}p_r e_{2-r} & e_1 & 1 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & 1 & 0 \\
      \vdots & \vdots & \ddots & \ddots  & e_1 & 1 \\            
      \sum\limits_{r=2}^{n}(-1)^{r-1}p_r e_{n-r} & e_{n-1} & e_{n-2} & \cdots  & e_2 & e_1 \\
    \end{vmatrix}
    \\
    &=
    \begin{vmatrix}
      0 & 1 & 0 & 0 & \cdots & 0 \\
      \sum\limits_{r=2}^{2}(-1)^{r-1}p_r e_{2-r} & e_1 & 1 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & 1 & 0 \\
      \vdots & \vdots & \ddots & \ddots  & e_1 & 1 \\            
      \sum\limits_{r=2}^{n}(-1)^{r-1}p_r e_{n-r} & e_{n-1} & e_{n-2} & \cdots  & e_2 & e_1 \\
    \end{vmatrix} \\    
    &= \dots
    \\
    &=
    \begin{vmatrix}
      0 & 1 & 0 & 0 & \cdots & 0 \\
      0 & e_1 & 1 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & 1 & 0 \\
      \vdots & \vdots & \ddots & \ddots  & e_1 & 1 \\            
      (-1)^{n-1}p_n & e_{n-1} & e_{n-2} & \cdots  & e_2 & e_1 \\
    \end{vmatrix} \\
    &=
    p_n
    \begin{vmatrix}
      1 & 0 & 0 & \cdots & 0 \\
       e_1 & 1 & 0 & \cdots & 0 \\
       \vdots & \ddots & \ddots  & \ddots & \vdots \\
       \vdots & \ddots & \ddots  & 1 & 0 \\
       \vdots & \ddots & \ddots  & e_1 & 1 
    \end{vmatrix}
    \\
    &=
    p_n
  \end{align*}
  Next, from Newton's identity we get
  \begin{align*}
    p_n = (-1)^{n-1}ne_n + \sum\limits_{r=1}^{n-1}(-1)^{n+r-1}p_r e_{n-r}
  \end{align*}
  so
  \begin{align*}
    \begin{vmatrix}
      p_1 & 1 & 0 & 0 & \cdots & 0 \\
      p_2 & p_1 & 2 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & 0 \\
      p_{n-1} & p_{n-2} & \cdots & \cdots  & p_1 & n-1 \\            
      p_n & p_{n-1} & \cdots & \cdots  & p_2 & p_1 
    \end{vmatrix} 
    &=
    \begin{vmatrix}
      e_1 & 1 & 0 & 0 & \cdots & 0 \\
      e_1p_1 - 2e_2 & p_1 & 2 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots
      & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & 0 \\
      (-1)^{n-2}(n-1)e_{n-1} + \sum\limits_{r=1}^{n-2}(-1)^{n+r-2}p_r e_{n-r-1} & p_{n-2} & \cdots & \cdots  & p_1 & n-1 \\            
      (-1)^{n-1}ne_n + \sum\limits_{r=1}^{n-1}(-1)^{n+r-1}p_r e_{n-r} & p_{n-1} & \cdots & \cdots  & p_2 & p_1 
    \end{vmatrix}
    \\
    &=
    e_1
    \begin{vmatrix}
      1 & 1 & 0 & 0 & \cdots & 0 \\
      p_1 & p_1 & 2 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots
      & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & 0 \\
      p_{n-2} & p_{n-2} & \cdots & \cdots  & p_1 & n-1 \\            
      p_{n-1} & p_{n-1} & \cdots & \cdots  & p_2 & p_1 
    \end{vmatrix} \\
    &+
    \begin{vmatrix}
      0 & 1 & 0 & 0 & \cdots & 0 \\
      - 2e_2 & p_1 & 2 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots
      & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & 0 \\
      (-1)^{n-2}(n-1)e_{n-1} + \sum\limits_{r=1}^{n-3}(-1)^{n+r-2}p_r e_{n-r-1} & p_{n-2} & \cdots & \cdots  & p_1 & n-1 \\            
      (-1)^{n-1}ne_n + \sum\limits_{r=1}^{n-2}(-1)^{n+r-1}p_r e_{n-r} & p_{n-1} & \cdots & \cdots  & p_2 & p_1 
    \end{vmatrix} \\
    &=
    \begin{vmatrix}
      0 & 1 & 0 & 0 & \cdots & 0 \\
      - 2e_2 & p_1 & 2 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots
      & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & 0 \\
      (-1)^{n-2}(n-1)e_{n-1} + \sum\limits_{r=1}^{n-3}(-1)^{n+r-2}p_r e_{n-r-1} & p_{n-2} & \cdots & \cdots  & p_1 & n-1 \\            
      (-1)^{n-1}ne_n + \sum\limits_{r=1}^{n-2}(-1)^{n+r-1}p_r e_{n-r} & p_{n-1} & \cdots & \cdots  & p_2 & p_1 
    \end{vmatrix} \\
    &=\dots \\
    &=
    \begin{vmatrix}
      0 & 1 & 0 & 0 & \cdots & 0 \\
      0 & p_1 & 2 & 0 & \cdots & 0 \\
      \vdots & \vdots & \ddots
      & \ddots  & \ddots & \vdots \\
      \vdots & \vdots & \ddots & \ddots  & \ddots & 0 \\
      0 & p_{n-2} & \cdots & \cdots  & p_1 & n-1 \\            
      (-1)^{n-1}ne_n & p_{n-1} & \cdots & \cdots  & p_2 & p_1 
    \end{vmatrix} \\
    &=
    ne_n
    \begin{vmatrix}
      1 & 0 & 0 & \cdots & 0 \\
      p_1 & 2 & 0 & \cdots & 0 \\
       \vdots & \ddots & \ddots  & \ddots & \vdots \\
      \vdots & \ddots & \ddots  & \ddots & 0 \\
      p_{n-2} & \cdots & \cdots  & p_1 & n-1 \\            
    \end{vmatrix} \\
    &=
    ne_n(n-1)! \\
    &= n!e_n
  \end{align*}
\end{proof}
\newpage
\begin{exercise}
  Write the skew Schur function $s_{(4,3,2)/(1,1)}$ as a linear combination of Schur functions. 
\end{exercise}
\begin{proof}
   By Equation (3.38) we have 
  \begin{align*}
    s_{\lambda/\mu} = \sum\limits_{\nu \in Par} c_{\mu \nu}^{\lambda} s_{\nu},
  \end{align*}
  and by Theorem 3.6.9, $c_{\mu \nu}^{\lambda}$ is the number of Littlewood-Richardson tableaux of shape $\lambda/\mu$ and weight $\nu$. Additionally, we use that a tableau is a Littlewood-Richardson tableau if and only if $word(T)$ is a lattice word (Prop 3.6.12). Since $s_{(4,3,2)/(1,1)}$ is homogeneous of degree 7, we compute $c_{\mu \nu}^{\lambda}$, for $\nu \in Par(7)$.

  Let $\lambda = (4,3,2)$ and $\mu = (1,1)$:
  
  \begin{itemize}
  \item
    $\nu = (6,1)$:
    
    No semistandard tableau of this weight, so $c_{\mu \nu}^{\lambda} = 0$.
  \item
    $\nu = (5,2)$:
    
    No semistandard tableau of this weight, so $c_{\mu \nu}^{\lambda} = 0$.    

  \item
    $\nu = (5,1^2)$:

    No semistandard tableau of this weight, so $c_{\mu \nu}^{\lambda} = 0$.    
    
  \item
    $\nu = (4,3)$:

    No semistandard tableau of this weight, so $c_{\mu \nu}^{\lambda} = 0$.    
    
  \item 
    $\nu =  (4,2,1)$:

    There's only one semi standard tableau of this weight, which is also a lattice word:
    \begin{align*}
      \begin{ytableau}
        \none & 1 & 1 & 1 \\
        \none & 2 & 2 \\
        1 & 3
      \end{ytableau}\quad,\quad
    \end{align*}
    $c_{\mu \nu}^{\lambda} = 1 $.

  \item 
    $\nu = (4,1^3)$:

    There are two semi standard tableau of this weight, both of which are not lattice words (in the first word a $3$ appears before a $2$, in the second a $4$ before a $2$) :
    \begin{align*}
      \begin{ytableau}
        \none & 1 & 1 & 1 \\
        \none & 2 & 3 \\
        1 & 4
      \end{ytableau}\quad,\quad
      \begin{ytableau}
        \none & 1 & 1 & 1 \\
        \none & 2 & 4 \\
        1 & 3
      \end{ytableau}\quad,\quad      
    \end{align*}
    so $c_{\mu \nu}^{\lambda} = 0$.    
  \item 
    $\nu = (3^2,1)$:

    There are two semi standard tableau of this weight, only the first of which corresponds to a lattice word (the second does not correspond to a lattice word since in it's word there's something either than a $1$ in the top right box):
    \begin{align*}
      \begin{ytableau}
        \none & 1 & 1 & 1 \\
        \none & 2 & 2 \\
        2 & 3
      \end{ytableau}\quad,\quad
      \begin{ytableau}
        \none & 1 & 1 & 2 \\
        \none & 2 & 2 \\
        1 & 3
      \end{ytableau}\quad,\quad      
    \end{align*}
    so $c_{\mu \nu}^{\lambda} = 1$.
    
  \item 
    $\nu = (3,2^2)$:
    
    \begin{align*}
      \begin{ytableau}
        \none & 1 & 1 & 1 \\
        \none & 2 & 2 \\
        3 & 3
      \end{ytableau}\quad,\quad
      \begin{ytableau}
        \none & 1 & 1 & 2 \\
        \none & 2 & 3 \\
        1 & 3
      \end{ytableau}\quad,\quad      
      \begin{ytableau}
        \none & 1 & 1 & 3 \\
        \none & 2 & 2 \\
        1 & 3
      \end{ytableau}\quad,\quad
      \begin{ytableau}
        \none & 1 & 1 & 1 \\
        \none & 2 & 3 \\
        2 & 3
      \end{ytableau}\quad,\quad      
    \end{align*}
    The first tableau corresponds to a lattice word, the second and third have something other than a $1$ in the top right box so don't correspond to lattice words, and in the word of the fourth a $3$ will appear before a $2$, so this last one isn't a lattice word either. We conclude $c_{\mu \nu}^{\lambda} = 1$.
    
  \item 
    $\nu = (3,2,1^2)$:

    The constraint that the word has to be a lattice word means that the first row must be $(1,1,1)$, the second $(2,2)$, but then we're left with the last row $(3,4)$. Since the first instance of a $4$ will appear before the first instance of a $3$ in the word, this is not a lattice word, and so there are no Littlewood-Richardson tableau of this weight: 
    \begin{align*}
      \begin{ytableau}
        \none & 1 & 1 & 1 \\
        \none & 2 & 2 \\
        3 & 4
      \end{ytableau}\quad,
    \end{align*}    
    $c_{\mu \nu}^{\lambda} = 0$.
    
  \item 
    $\nu = (3,1^4)$:

    First row $1$s, second row will have to be strictly increasing (as only distinct numbers remain), so the first instance of a greater number will appear before some lesser number, which therefore cannot be a lattice word, so $c_{\mu \nu}^{\lambda} = 0$.
    
  \item 
    $\nu = (2^3,1)$:

    Top right box is not 1, so no lattice words, so $c_{\mu \nu}^{\lambda} = 0 $.
  \item 
    $\nu = (2^2,1^3)$:

    Top right box is not 1, so no lattice words, so $c_{\mu \nu}^{\lambda} = 0 $.   
    
  \item 
    $\nu = (2,1^5)$:

    Top right box is not 1, so no lattice words, so $c_{\mu \nu}^{\lambda} = 0 $.   
  \item 
    $\nu = (1^7)$:

    Top right box is not 1, so no lattice words, so $c_{\mu \nu}^{\lambda} = 0 $.    
  \end{itemize}

  Putting it all together, we get
  \begin{align*}
    s_{\lambda/\mu} = s_{(4,2,1)} + s_{(3,3,1)} + s_{(3,2,2)}.
  \end{align*}
\end{proof}

\begin{exercise}
  Prove that
  \begin{align*}
    \set{f^{\perp} : f \in Sym_\Q} = Span_{\Q}\set{\cfrac{\partial^k}{\partial p_{i_1} \partial p_{i_2} \cdots \partial p_{i_k}} : k \in \N, i_1, \dots, i_k \geq 1 }.
  \end{align*}
  Here we view both sides as subspaces of $End_{\Q}(Sym)$.
\end{exercise}

\begin{proof}
  By Theorem 2.6.4., we know that the $p_\lambda$ ($\lambda \in Par$) form a $\Q$-basis of $Sym_\Q$. By Proposition 4.2.7 we have
  \begin{align*}
    p_n^{\perp} = n \cfrac{\partial}{\partial p_n},
  \end{align*}
  We use the assumption that the mixed partials commute in their action on $Sym_\Q$.
  \footnote{We have $p_n^- h_m = h_{m-n}$, so

  \begin{align*}
    p_n^-p_m^- h_{\lambda}
    &=
    \sum\limits_{i,j=1}^{\ell(\lambda)} h_{\lambda_1} \cdots \cfrac{\partial}{\partial p_n} h_{\lambda_i} \cdots \cfrac{\partial}{\partial p_m} h_{\lambda_j} \cdots h_{\lambda_{\ell{\lambda}}} \\
    &=
    \sum\limits_{i,j=1}^{\ell(\lambda)} h_{\lambda_1} \cdots h_{\lambda_i-n} \cdots h_{\lambda_j-m} \cdots h_{\lambda_{\ell{\lambda}}} \\    
    &=
    p_m^-(\sum\limits_{i,j=1}^{\ell(\lambda)} h_{\lambda_1} \cdots h_{\lambda_i-n} \cdots h_{\lambda_j} \cdots h_{\lambda_{\ell{\lambda}}}) \\
    &=
    p_m^-p_n^-(\sum\limits_{i,j=1}^{\ell(\lambda)} h_{\lambda_1} \cdots h_{\lambda_i} \cdots h_{\lambda_j} \cdots h_{\lambda_{\ell{\lambda}}}) \\
    &=
    p_m^-p_n^-h_{\lambda},\\
  \end{align*}
  and $\set{h_{\lambda} : \lambda \in Par}$ forms a $\Q$-basis of $Sym_{\Q}$.}
  By Prop 4.2.1 we know that the adjoint is a ring morphism, so
  \begin{align*}
    p_{\lambda}^{\perp} &=
    (p_{\lambda_1} \cdots p_{\lambda_{\ell(\lambda)}})^{\perp} \\
    &= p_{\lambda_1}^{\perp} \cdots p_{\lambda_{\ell(\lambda)}}^{\perp} \\
    &= \prod_{i=1}^{\ell(\lambda)} \lambda_i \cfrac{\partial}{\partial p_{\lambda_i}} \\
    &= a_{\lambda} \cfrac{\partial^{\ell(\lambda)}}{\partial p_{\lambda_1}\partial p_{\lambda_2} \cdots \partial p_{\lambda_{\ell(\lambda)}}},
  \end{align*}
  for $a_{\lambda} = \prod\limits_{i=1}^{\ell(\lambda)} \lambda_i$.

  Now, let $f^{\perp}$ be an element on the left hand side. We have for some $b_\lambda \in \Q$ that $f = \sum\limits_{\lambda \in Par} b_{\lambda} p_{\lambda}$, and so using again that the adjoint is a ring morphism we get:
  \begin{align*}
    f^{\perp}
    &=
    (\sum\limits_{\lambda \in Par} b_{\lambda} p_{\lambda})^{\perp}
    \\
    &=
    \sum\limits_{\lambda \in Par} b_{\lambda} p_{\lambda}^{\perp}    \\
    &=
    \sum\limits_{\lambda \in Par} (b_{\lambda} a_{\lambda}) \cfrac{\partial^{\ell(\lambda)}}{\partial p_{\lambda_1}\partial p_{\lambda_2} \cdots \partial p_{\lambda_{\ell(\lambda)}}}, 
  \end{align*}
  where $a_{\lambda} = \prod\limits_{i=1}^{\ell(\lambda)} \lambda_i$, and $b_{\lambda} a_{\lambda} \in \Q$; and so $f^{\perp}$ is an element of the right hand side.

  For the opposite inclusion, we define a partition for each ordered $i \in \N^k$, $k \in \N$:
  \begin{align*}
    \lambda^i
    &= i \text{ permuted into a partition } \\
    &= i^{\pi},
  \end{align*}
  for some $\pi \in \mathfrak{S}_k$ such that $i^{\pi} \in Par$. This is well defined, since each such ordered $i$ is a weak composition, so there exists a unique partition into which it may be permuted.

  We have
  \begin{align*}
    \cfrac{\partial^k}{\partial p_{i_1} \partial p_{i_2} \cdots \partial p_{i_k}} = \cfrac{1}{a_{\lambda^i}} (p_{\lambda^i})^{\perp},
  \end{align*}
  where
  \begin{align*}
    a_{\lambda^i} = i_1 i_2 \cdots i_k.
  \end{align*}
  For $b_{i} \in \Q$ we have
  \begin{align*}
    \sum\limits_{i} b_{i}\cfrac{\partial^k}{\partial p_{i_1} \partial p_{i_2} \cdots \partial p_{i_k}}
    &=
    \sum\limits_{i} b_{i}\cfrac{1}{a_{\lambda^i}} (p_{\lambda^i})^{\perp} \\
    &=
    (\sum\limits_{i} b_{i}\cfrac{1}{a_{\lambda^i}}p_{\lambda^i})^{\perp},
  \end{align*}
  where the sum is over ordered $i=(i_1, \dots, i_k) \in \N^k$, $k \in \N$. Since $\sum\limits_{i} b_{i}\cfrac{1}{a_{\lambda^i}}p_{\lambda^i} \in Sym_{\Q}$, this $\Q$-linear combination from the right hand side is included in the left hand side.
\end{proof}

\begin{exercise}
  Suppose $(C, \triangle, \epsilon)$ is a counital coassociative coalgebra over a field $k$. Show that the dual space $C^* = Hom_k(C,k)$ is a unital associative algebra with multiplication
  \begin{align*}
    \nabla: C^* \otimes C^* \to C^*, \quad \nabla(f \otimes g)(x) = (f \otimes g) \circ \triangle(x), \quad f, g \in C^*, \quad x \in C,
  \end{align*}
  extended by linearity, and a unit
  \begin{align*}
    \eta: k \to C^* , \quad \eta(a) = a\epsilon, \quad a \in k.
  \end{align*}
\end{exercise}
\begin{proof}
  Let $f,g,h \in C^*$ and $x \in C$, then 
  \begin{align*}
    (\nabla \circ \nabla \otimes id)(f \otimes g \otimes h)(x)
    &= \nabla(\nabla(f \otimes g) \otimes h)(x)\\
    &= (\nabla(f \otimes g) \otimes h) \circ \triangle(x)\\
    &= (f \otimes g \otimes h) \circ \triangle \otimes id \circ \triangle(x)\\
    &= (f \otimes g \otimes h) \circ id \otimes \triangle \circ \triangle(x)\\
    &= (f \otimes \nabla(g \otimes h))\circ \triangle(x)\\
    &= \nabla(f \otimes \nabla(g \otimes h))(x) \\
    &= (\nabla\circ id \otimes \nabla)(f \otimes g \otimes h)(x),    
  \end{align*}
  where we used the coassociativity of $\triangle$ in the fourth equality. We conclude that
  \begin{align*}
    \nabla \circ \nabla \otimes id = \nabla\circ id \otimes \nabla,
  \end{align*}
  and so the first diagram of 4.17 commutes.

  Next, let $a \in k$, $f \in C^*$, then
  \begin{align*}
    (\nabla \circ \eta \otimes id)(a \otimes f)(x)
    &=
    \nabla(a \epsilon \otimes f)(x) \\
    &=
    (a \epsilon \otimes f) \circ \triangle (x) \\
    &=
    (a \otimes f)\circ \epsilon \otimes id \circ \triangle (x) \\
    &=
    (f \otimes a) \circ id \otimes \epsilon \circ \triangle (x) \\
    &=
    (f \otimes a \epsilon)  \circ \triangle (x) \\
    &=
    \nabla(f \otimes a \epsilon) (x) \\
    &= (\nabla \circ id \otimes \eta)(f \otimes a) (x),
  \end{align*}
  where we used the commutativity of the counit diagram in the fourth equality. Identifying $a \otimes f$, $f \otimes a$  with $af$ (by $a \otimes f \overset{\cong}{\mapsto} af$ and $f \otimes a \overset{\cong}{\mapsto} af$) we conclude that
  \begin{align*}
    \nabla \circ \eta \otimes id = \nabla \circ id \otimes \eta,
  \end{align*}
  and so the second diagram of 4.17 commutes.
\end{proof}
\newpage
\begin{exercise}
  Let $m,n \in \N$. In the Heisenberg algebra, write $p_m^-e_n^+$ and $p_m^-h_n^+$ as linear combinations of elements of the form $f^+g^-$, $f,g \in Sym$.
\end{exercise}
\begin{proof}
  \begin{align*}
    \triangle p_m = 1 \otimes p_m + p_m \otimes 1,
  \end{align*}
  so using Prop 5.1.3 we have 
  \begin{align*}
    p_m^- e_n^+
    &=
    e_n^+p_m^- + (p_m^-e_n)^+1^- \\
    &=
    e_n^+p_m^- + ((-1)^{m-1}e_{n-m})^+1^-,
  \end{align*}
  where we used that $p_m^- e_n = (-1)^{m-1}e_{n-m}$ (Prop 4.2.9).

  Next,
  \begin{align*}
    p_m^-h_n^+
    &=
    h_n^+p_m^- + (p_m^-h_n)^+1^- \\
    &= h_n^+p_m^- + h_{n-m}^+1^-,
  \end{align*}
  where we used that $p_m^-h_n = h_{n-m}$ (Prop 4.2.9).
\end{proof}
\begin{exercise}
  Let $H$ be a subgroup of a finite group $G$, and let $f \in \mathcal{C}(H,\C)$. Define
  \begin{align*}
    \hat f : G \to \C, \quad \hat f (x) = 
    \begin{cases}
      f(x) & x \in H, \\
      0 & x \not \in H. 
    \end{cases}
  \end{align*}
  \begin{enumerate}
  \item
    Show that
    \begin{align*}
      \hat f (y^{-1} z y) = \hat f (z), \quad \text{ for all } y \in H, z \in G. 
    \end{align*}
  \item
    Let $x_1, \dots, x_m$ be a complete set of representatives of the left cosets of $H$ in $G$. Recall that we defined
    \begin{align*}
      Ind_H^G(f)(x) = \cfrac{1}{\abs{H}} \sum\limits_{y \in G} \hat f (y^{-1}xy).
    \end{align*}
    Prove that
    \begin{align*}
      Ind_H^G(f)(x) = \sum\limits_{i=1}^m \hat f (x_i^{-1}xx_i).      
    \end{align*}
  \end{enumerate}
\end{exercise}
\begin{proof}
  \begin{enumerate}
  \item
    Fix $y$ in $H$. If $y^{-1}zy \in H$, then $ z \in yHy^{-1}=H$, so
    \begin{align*}
      \hat f(y^{-1}zy) = f(y^{-1}zy) = f(z) = \hat f (z),
    \end{align*}
    since $f \in \mathcal{C}(H,\C)$. Otherwise, $y^{-1}zy \not \in H$, so $z \not \in H$ and
    \begin{align*}
      \hat f(y^{-1}zy) = \hat f(z) = 0.
    \end{align*}
  \item
    We have $G = \bigsqcup\limits_{i=1}^m x_iH$, so
    \begin{align*}
      Ind_H^G(f)(x)
      &=
      \cfrac{1}{\abs{H}} \sum\limits_{y \in G}\hat f (y^{-1}xy) \\
      &=
      \cfrac{1}{\abs{H}} \sum\limits_{i=1}^m \sum\limits_{y \in H}\hat f ((x_i y)^{-1}x x_i y) \\
      &= 
      \cfrac{1}{\abs{H}} \sum\limits_{i=1}^m \sum\limits_{y \in H}\hat f (y^{-1} x_i^{-1}x x_i y) \\
      &=
      \cfrac{1}{\abs{H}} \sum\limits_{i=1}^m \sum\limits_{y \in H}\hat f (x_i^{-1}x x_i) \\
      &=
      \sum\limits_{i=1}^m f (x_i^{-1}x x_i),                         
    \end{align*}
    where in the second last equality we used the result from the first part of this question.
  \end{enumerate}
\end{proof}
\newpage
\begin{exercise}
  Use Proposition 6.3.9 to compute the character table of $\mathfrak{S}_4$.
\end{exercise}
\begin{proof}
  Let $\lambda, \mu \in Par$. We list the borderstrip tableau $T \in BST(\lambda, \mu)$, and calculate the character table entries.

  \begin{itemize}
  \item
    $\lambda=(4)$:
    \begin{itemize}
    \item
      $\mu = (4)$:
      
      \begin{align*}
        \begin{ytableau}
          1 & 1 & 1 & 1
        \end{ytableau}\quad,
      \end{align*}
      $\chi_{\mu}^{\lambda} = (-1)^0 = 1$.

    \item
      $\mu = (3,1)$:

      \begin{align*}
        \begin{ytableau}
          1 & 1 & 1 & 2\\
        \end{ytableau}\quad,
      \end{align*}
      $\chi_{\mu}^{\lambda} = (-1)^{0+0} = 1$.

    \item
      $\mu = (2,2)$:

      \begin{align*}
        \begin{ytableau}
          1 & 1 & 2 & 2\\
        \end{ytableau}\quad,
      \end{align*}
      $\chi_{\mu}^{\lambda} = (-1)^{0+0} = 1$.    

    \item
      $\mu = (2,1,1)$:

      \begin{align*}
        \begin{ytableau}
          1 & 1 & 2 & 3\\
        \end{ytableau}\quad,
      \end{align*}
      $\chi_{\mu}^{\lambda} = (-1)^{0+0+0} = 1$.

    \item
      $\mu = (1^4)$:

      \begin{align*}
        \begin{ytableau}
          1 & 2 & 3 & 4\\
        \end{ytableau}\quad,
      \end{align*}
      $\chi_{\mu}^{\lambda} = (-1)^{0+0+0+0} = 1$.
    \end{itemize}
    
    We have then the first row of our character table $\chi^{\lambda} = (1,1,1,1,1)$.    
  \item
    $\lambda = (3,1)$:
    
    \begin{itemize}
      
    \item
      $\mu = (4)$:
      
      \begin{align*}
        \begin{ytableau}
          1 & 1 & 1 \\
          1
        \end{ytableau}\quad,
      \end{align*}
      $\chi_{\mu}^{\lambda}  = (-1)^{1} = -1$.

    \item
      $\mu = (3,1)$:
      
      \begin{align*}
        \begin{ytableau}
          1 & 1 & 1 \\
          2
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 1 & 2 \\
          1
        \end{ytableau}\quad,        
      \end{align*}
      $\chi_{\mu}^{\lambda}  = (-1)^{0+0} + (-1)^{1 + 0} = 0$.

    \item
      $\mu = (2,2)$:
      
      \begin{align*}
        \begin{ytableau}
          1 & 2 & 2 \\
          1
        \end{ytableau}\quad,        
      \end{align*}
      $\chi_{\mu}^{\lambda}  = (-1)^{1+0} = -1$.

    \item
      $\mu = (2,1,1)$:
      
      \begin{align*}
        \begin{ytableau}
          1 & 1 & 2 \\
          3
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 1 & 3 \\
          2
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 2 & 3 \\
          1
        \end{ytableau}\quad,                
      \end{align*}
      $\chi_{\mu}^{\lambda}  = (-1)^{0+0+0} + (-1)^{0+0+0} + (-1)^{1+0+0}= 1$.

    \item
      $\mu = (1^4)$:
      
      \begin{align*}
        \begin{ytableau}
          1 & 3 & 4 \\
          2
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 2 & 3 \\
          4
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 2 & 4 \\
          3
        \end{ytableau}\quad,                
      \end{align*}
      $\chi_{\mu}^{\lambda}  = (-1)^{0+0+0+0} + (-1)^{0+0+0+0} +(-1)^{0+0+0+0} = 3$.

    \end{itemize}
    The second row is $\chi^{\lambda} = (-1,0,-1,1,3). $    

  \item
    $\lambda = (2,2)$:
    
    \begin{itemize}
      
    \item
      $\mu = (4)$:
      
      $BST(\lambda,\mu) = \emptyset$, so $\chi_{\mu}^{\lambda} = 0$.

    \item
      $\mu = (3,1)$:
      \begin{align*}
        \begin{ytableau}
          1 & 1 \\
          1 & 2
        \end{ytableau}\quad,\quad
      \end{align*}
      
      $\chi_{\mu}^{\lambda} = (-1)^{1+0} = -1$.

    \item
      $\mu = (2,2)$:
      \begin{align*}
        \begin{ytableau}
          1 & 1 \\
          2 & 2 
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 2 \\
          1 & 2 
        \end{ytableau}\quad,\quad        
      \end{align*}

      $\chi_{\mu}^{\lambda} = (-1)^{0 + 0} + (-1)^{1 + 1} = 2$.

    \item
      $\mu = (2,1,1)$:
      \begin{align*}
        \begin{ytableau}
          1 & 1 \\
          2 & 3 
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 2 \\
          1 & 3 
        \end{ytableau}\quad,\quad        
      \end{align*}
      
      $\chi_{\mu}^{\lambda} = (-1)^{0 + 0 + 0} + (-1)^{1 + 0 + 0} = 0$.

    \item
      $\mu = (1^4)$:
      \begin{align*}
        \begin{ytableau}
          1 & 3 \\
          2 & 4 
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 2 \\
          3 & 4 
        \end{ytableau}\quad,\quad
      \end{align*}
      
      $\chi_{\mu}^{\lambda} = (-1)^{0 + 0 + 0 + 0} + (-1)^{0 + 0 + 0 + 0} = 2$.      
    \end{itemize}
    The third row is $\chi^\lambda = (0,-1,2,0,2)$.

  \item
    $\lambda = (2,1,1)$:
    
    \begin{itemize}
      
    \item
      $\mu = (4)$:
      \begin{align*}
        \begin{ytableau}
          1 & 1 \\
          1 \\
          1
        \end{ytableau}\quad,
      \end{align*}
      $\chi_{\mu}^{\lambda} = (-1)^{2} = 1$.

    \item
      $\mu = (3,1)$:
      \begin{align*}
        \begin{ytableau}
          1 & 2 \\
          1 \\
          1
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 1 \\
          1 \\
          2
        \end{ytableau}\quad,        
      \end{align*}
      $\chi_{\mu}^{\lambda} = (-1)^{2+0} + (-1)^{1+0} = 0$.

    \item
      $\mu = (2,2)$:
      \begin{align*}
        \begin{ytableau}
          1 & 1 \\
          2 \\
          2
        \end{ytableau}\quad,        
      \end{align*}
      $\chi_{\mu}^{\lambda} = (-1)^{0+1}= -1$.

    \item
      $\mu = (2,1,1)$:
      \begin{align*}
        \begin{ytableau}
          1 & 3 \\
          1 \\
          2
        \end{ytableau}\quad,\quad                        
        \begin{ytableau}
          1 & 2 \\
          1 \\
          3
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 1 \\
          2 \\
          3
        \end{ytableau}\quad,\quad
      \end{align*}
      $\chi_{\mu}^{\lambda} = (-1)^{1+0+0} + (-1)^{1+0+0} + (-1)^{0 + 0 +0}= -1$.

    \item
      $\mu = (1^4)$:
      \begin{align*}
        \begin{ytableau}
          1 & 4 \\
          2 \\
          3
        \end{ytableau}\quad,\quad                        
        \begin{ytableau}
          1 & 3 \\
          2 \\
          4
        \end{ytableau}\quad,\quad
        \begin{ytableau}
          1 & 2 \\
          3 \\
          4
        \end{ytableau}\quad,\quad
      \end{align*}
      $\chi_{\mu}^{\lambda} = (-1)^{0+0+0+0} +  (-1)^{0+0+0+0} +  (-1)^{0+0+0+0}= 3$.            
    \end{itemize}
    The fourth row is $\chi^{\lambda} = (1,0,-1,-1,3)$.

  \item
    $\lambda = (1^4)$:
    \begin{itemize}
    \item
      $\mu = (4)$:
      \begin{align*}
        \begin{ytableau}
          1 \\
          1 \\
          1 \\
          1 \\
        \end{ytableau}\quad,
      \end{align*}
      $\chi_\mu^{\lambda} = (-1)^3 = -1$.

    \item
      $\mu = (3,1)$:
      \begin{align*}
        \begin{ytableau}
          1 \\
          1 \\
          1 \\
          2 \\
        \end{ytableau}\quad,
      \end{align*}
      $\chi_\mu^{\lambda} = (-1)^{2+0} = 1$.

    \item
      $\mu = (2,2)$:
      \begin{align*}
        \begin{ytableau}
          1 \\
          1 \\
          2 \\
          2 \\
        \end{ytableau}\quad,
      \end{align*}
      $\chi_\mu^{\lambda} = (-1)^{1+1} = 1$.

    \item
      $\mu = (2,1,1)$:
      \begin{align*}
        \begin{ytableau}
          1 \\
          1 \\
          2 \\
          3 \\
        \end{ytableau}\quad,
      \end{align*}
      $\chi_\mu^{\lambda} = (-1)^{1+0+0} = -1$.

    \item
      $\mu = (1^4)$:
      \begin{align*}
        \begin{ytableau}
          1 \\
          2 \\
          3 \\
          4 \\
        \end{ytableau}\quad,
      \end{align*}
      $\chi_\mu^{\lambda} = (-1)^{0+0+0+0} = 1$.                        
    \end{itemize}
  \item
    The fifth row is $\chi^{\lambda} = (-1,1,1,-1,1)$.
  \end{itemize}

  We put our rows into a table:
  \begin{align*}
    \begin{tabular}{|c|c|c|c|c|c|}
      \hline
      & (4) & (3,1) & (2,2)& (2,1,1) & (1^4)      \\
      \hline
      \chi^{(4)} & 1 & 1 & 1 & 1 & 1 \\
      \hline
      \chi^{(3,1)} & -1 & 0 & -1 & 1 & 3 \\
      \hline
      \chi^{(2,2)} & 0 & -1 & 2 & 0 & 2 \\
      \hline
      \chi^{(2,1,1)} & 1 & 0 & -1 & -1 & 3 \\
      \hline      
      \chi^{(1^4)} & -1 & 1 & 1 & -1 & 1 \\
      \hline
    \end{tabular}\quad.
  \end{align*}
\end{proof}
\end{document}
